{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2dbff4d-e3d3-434a-8360-a14b5f4b4342",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/contreras-juan/UPTC_Diplomado_Ciencia_de_Datos/blob/main/metodos_supervisados/Validaciones.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3c3d4-d16d-4a6e-ba4b-e46e448425de",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://storage.googleapis.com/adonaivera_certified/banner.png\">\n",
    "    \n",
    "##  Diplomado en Ciencia de Datos - Cohorte 2024\n",
    "\n",
    "Autor: [Juan Felipe Contreras](https://www.linkedin.com/in/juanf-contreras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64559b-2c11-4cbb-81c2-1e983140c032",
   "metadata": {},
   "source": [
    "<h1 align = 'center'> Validaciones cruzadas y búsqueda de hiperparámetros </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c11bec9-2b4a-4a2b-8c5b-75372140d026",
   "metadata": {},
   "source": [
    "La validación cruzada y la búsqueda de hiperparámetros son técnicas esenciales en el campo del aprendizaje de máquinas para garantizar el rendimiento óptimo de un modelo. La validación cruzada es un método para evaluar el rendimiento de un modelo al dividir los datos en conjuntos de entrenamiento y prueba múltiples veces, lo que permite una evaluación más robusta de su capacidad predictiva. Por otro lado, la búsqueda de hiperparámetros implica encontrar la combinación óptima de parámetros del modelo que maximice su rendimiento predictivo. Ambos procesos son fundamentales para garantizar que un modelo de aprendizaje profundo pueda generalizar bien a datos nuevos y desconocidos, maximizando su utilidad en aplicaciones del mundo real.\n",
    "\n",
    "En el enfoque tradicional, dividimos la muestra total de los datos en un conjunto de entrenamiento y uno de prueba; entrenamos los datos con el primer conjunto y lo evaluamos con el segundo. Esta metodología presenta algunas debilidades; la principal es el sesgo de muestra, lo que puede ocasionar que la distribución de alguna variable en los datos de entrenamiento no sea cercana a la distribución de la variable en la muestra total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63c8ac5-a873-4742-a6db-fde5ba918384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55dff66a-9074-4a34-809d-6451453d7539",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd06134-2679-44d5-868f-bb9b017b8ba4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m categories \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSetosa\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVersicolor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVirgínica\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      6\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      8\u001b[0m frequencies \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_train, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "categories = ['Setosa', 'Versicolor', 'Virgínica']\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(15, 5))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "frequencies = np.unique(y_train, return_counts=True)[1]\n",
    "\n",
    "ax[0].bar(categories, frequencies)\n",
    "ax[0].set_title('Distribución sin estratificación')\n",
    "ax[0].set_ylabel('Frecuencia')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "frequencies = np.unique(y_train, return_counts=True)[1]\n",
    "\n",
    "ax[1].bar(categories, frequencies)\n",
    "ax[1].set_title('Distribución con estratificación')\n",
    "ax[1].set_ylabel('Frecuencia');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b5477e-6055-4312-ab38-72a6c92dae56",
   "metadata": {},
   "source": [
    "<h2> Dejar uno por fuera (Leave-One-Out) </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e6da8b-853a-49b7-aacd-e78445e3090f",
   "metadata": {},
   "source": [
    "Otra estrategia adicional que puede emplearse es dividir los datos de entrenamiento en subconjuntos de tamaño $N$ y dividir cada subconjunto en su propio subconjunto de entrenamiento y prueba, en dónde esté último es un conjunto de un solo elemento. Es decir, sea $X_{N \\text{x} K}$ nuestros datos de entrenamiento, generamos $N$ particiones diferentes permitiendo que cada elemento de la muestra sea el dato de prueba una sola vez. Para cada iteración calculamos el error del modelo, y finalmente promediamos los errores.\n",
    "\n",
    "\n",
    "<img src = 'img/LOO.png'>\n",
    "\n",
    "Fuente: [Raschka and Mirjalili](https://www.buscalibre.com.co/libro-python-machine-learning-machine-learning-and-deep-learning-with-python-scikit-learn-and-tensorflow-2-3rd-edition/9781789955750/p/52197210)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f0d813-7d90-4120-b1c6-fc77fc474c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786df497-0903-4b3a-8524-2af0ca7ddab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "y_true, y_pred = list(), list()\n",
    "\n",
    "for train_ix, test_ix in loo.split(X):\n",
    "    X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "    y_train, y_test = y[train_ix], y[test_ix]\n",
    "    \n",
    "    # fit model\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # evaluate model\n",
    "    yhat = knn.predict(X_test)\n",
    "    \n",
    "    # store\n",
    "    y_true.append(y_test[0])\n",
    "    y_pred.append(yhat[0])\n",
    "    \n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print('Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7a6cb-3662-43be-b267-3db90fa61e1c",
   "metadata": {},
   "source": [
    "<h2> Dejar $p$ por fuera (Leave P Out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09044b72-9c78-4d54-a29f-d8963983ae4e",
   "metadata": {},
   "source": [
    "Un caso general de la técnica de dejar a uno por fuera es la de dejar $p$ por fuera, lo que consiste en replicar el mismo ejercicio anterior en el que los datos de prueba de cada subconjunto de los datos de entrenamiento es de tamaño $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87879257-a5bf-4a91-810b-e3598d882f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeavePOut\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2409291-1859-4231-b029-556a79b3c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "lpo = LeavePOut(p=2)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "y_true, y_pred = list(), list()\n",
    "\n",
    "for train_ix, test_ix in lpo.split(X):\n",
    "    X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "    y_train, y_test = y[train_ix], y[test_ix]\n",
    "    \n",
    "    # fit model\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # evaluate model\n",
    "    yhat = knn.predict(X_test)\n",
    "    \n",
    "    # store\n",
    "    y_true.append(y_test[0])\n",
    "    y_pred.append(yhat[0])\n",
    "    \n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "print('Tiempo de procesamiento:', end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb96cc1-c31c-425e-8a80-9c574fcc6480",
   "metadata": {},
   "source": [
    "<h2> K-Folds </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df6b7ee-7a6f-4575-95f2-a7e35ef246bf",
   "metadata": {},
   "source": [
    "Una generalización de las metodologías anteriores consiste en una validación cruzada K-Fold que consiste en en dividir los datos de entrenamiento entre $K$ particiones. Para cada una de ellas entrenamos el modelo con los $K - 1$ subconjuntos, y evaluamos con el $K-ésimo$. Note que si $K = N$, esta técnica coincide con Leave One Out.\n",
    "\n",
    "<img src='img/KCV.png'>\n",
    "\n",
    "Fuente: [Scikit-Learn](https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9131f4-c5d8-4929-b7a9-261f237497b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5c2236-ecee-4978-8198-a7f4ca651ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing cross validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "acc_score= []\n",
    "\n",
    "for train_ix , test_ix in kf.split(X):\n",
    "    X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "    y_train, y_test = y[train_ix], y[test_ix]\n",
    "     \n",
    "    knn.fit(X_train,y_train)\n",
    "    \n",
    "    pred = knn.predict(X_test)\n",
    "     \n",
    "    acc = accuracy_score(pred, y_test)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    "\n",
    "print(f'Exactitud de cada folio - {acc_score}')\n",
    "print(f'Exactitud del modelo en los datos de entrenamiento: {avg_acc_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0342e-6e5f-4166-b966-a70c706606a8",
   "metadata": {},
   "source": [
    "<h2> Búsqueda de hiperparámetros </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ecc415-3ada-4459-897c-95ee03b8a0c0",
   "metadata": {},
   "source": [
    "Las técnicas vistas anteriormente funcionan bastante bien para encontrar hiperparámetros de los modelos, tal que probamos múltiples entrenamientos con valores diferentes de los hiperparámetros y escogemos la combinación que mejores resultados den."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d8c36b-dff6-4054-a3d3-f4a41de20b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32599d46-0cd0-461a-a9e4-06a7bff678c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris dataset as an example\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define range of k values to test\n",
    "k_values = list(range(1, 31))\n",
    "\n",
    "# Define an empty list to store cross-validation scores\n",
    "cv_scores = []\n",
    "\n",
    "# Perform 10-fold cross-validation for each k\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Find the optimal k\n",
    "optimal_k = k_values[np.argmax(cv_scores)]\n",
    "print(\"K óptimo:\", optimal_k)\n",
    "\n",
    "# Plot the accuracy vs k values\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(k_values, cv_scores)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Exactitud')\n",
    "plt.title('Valor óptimo de k para KNN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df16354c-c8fd-4957-9cd5-0eb6a8a06094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5801a4-9f6f-400c-8b58-b08e21e1747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "sorted(clf.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e793ea37-31b5-42f9-a07c-c0e6a00dcec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d1629-139d-4c0a-8dfc-9f1fc16ee20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f07f3-aaa9-4ca3-a3bb-e18bcb8bc189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04ab176-4e17-4623-9963-597ae542375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb59c54-1748-475e-9f29-c879c68e3ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac5900-a8ca-4c33-9816-12bff3ad8ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
